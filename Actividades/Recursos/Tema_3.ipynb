{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nombre: Jessica Méndez\n",
        "\n",
        "Materia: PLN, Visión y Ética Computacional\n",
        "\n",
        "02 de Febrero de 2023\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p_6p8KA0yC4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tecmilenio y la creadora del presente notebook, no guarda relación alguna con las marcas mencionadas como ejemplo. Las marcas son propiedad de sus titulares conforme a la legislación aplicable, se utilizan con fines académicos y didácticos, por lo que no existen fines de lucro, relación publicitaria o de patrocinio.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "HVLAEaFLyS1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NLTK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsDS9An5y6Px",
        "outputId": "29d2c127-231d-4f73-c8a4-911afe048fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from NLTK) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from NLTK) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from NLTK) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from NLTK) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de Paquetes"
      ],
      "metadata": {
        "id": "tkE5qqY7yVmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Q5EL-7OEkQss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt') \n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry7S9JOpkScV",
        "outputId": "d58c6903-93d6-4b7c-d466-105766e41571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cadenas"
      ],
      "metadata": {
        "id": "OeV1ude5kT81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"¡Hola Mundo! Hello world !       gr8\""
      ],
      "metadata": {
        "id": "3OKZ3t_hySJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(s.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTcKtzYJyhWL",
        "outputId": "a8f3f176-1395-47b0-b4bc-e2027bf23a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¡Hola', 'Mundo!', 'Hello', 'world', '!', 'gr8']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7A9YMSEzPSh",
        "outputId": "ab86051e-e920-49b0-9b74-32a307224984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['¡Hola', 'Mundo', '!', 'Hello', 'world', '!', 'gr8']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(s, pattern='\\w+')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17Qkn9KtzVs4",
        "outputId": "e307968c-08c7-4305-e2f6-19c84a4276f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola', 'Mundo', 'Hello', 'world', 'gr8']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(s, pattern='\\d+')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2gRkbgAzpCs",
        "outputId": "94783525-b114-4784-ac20-809a6bb9e08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEEdawdfzuag",
        "outputId": "4f61600a-561b-43e5-e947-7f0f70bc53f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['¡', 'Hola', 'Mundo', '!', 'Hello', 'world', '!', 'gr8']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_sp = stopwords.words('spanish')\n",
        "stop_words_sp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4X3rLgJ0BMe",
        "outputId": "897f411f-981c-47ea-b749-3666491b7ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de',\n",
              " 'la',\n",
              " 'que',\n",
              " 'el',\n",
              " 'en',\n",
              " 'y',\n",
              " 'a',\n",
              " 'los',\n",
              " 'del',\n",
              " 'se',\n",
              " 'las',\n",
              " 'por',\n",
              " 'un',\n",
              " 'para',\n",
              " 'con',\n",
              " 'no',\n",
              " 'una',\n",
              " 'su',\n",
              " 'al',\n",
              " 'lo',\n",
              " 'como',\n",
              " 'más',\n",
              " 'pero',\n",
              " 'sus',\n",
              " 'le',\n",
              " 'ya',\n",
              " 'o',\n",
              " 'este',\n",
              " 'sí',\n",
              " 'porque',\n",
              " 'esta',\n",
              " 'entre',\n",
              " 'cuando',\n",
              " 'muy',\n",
              " 'sin',\n",
              " 'sobre',\n",
              " 'también',\n",
              " 'me',\n",
              " 'hasta',\n",
              " 'hay',\n",
              " 'donde',\n",
              " 'quien',\n",
              " 'desde',\n",
              " 'todo',\n",
              " 'nos',\n",
              " 'durante',\n",
              " 'todos',\n",
              " 'uno',\n",
              " 'les',\n",
              " 'ni',\n",
              " 'contra',\n",
              " 'otros',\n",
              " 'ese',\n",
              " 'eso',\n",
              " 'ante',\n",
              " 'ellos',\n",
              " 'e',\n",
              " 'esto',\n",
              " 'mí',\n",
              " 'antes',\n",
              " 'algunos',\n",
              " 'qué',\n",
              " 'unos',\n",
              " 'yo',\n",
              " 'otro',\n",
              " 'otras',\n",
              " 'otra',\n",
              " 'él',\n",
              " 'tanto',\n",
              " 'esa',\n",
              " 'estos',\n",
              " 'mucho',\n",
              " 'quienes',\n",
              " 'nada',\n",
              " 'muchos',\n",
              " 'cual',\n",
              " 'poco',\n",
              " 'ella',\n",
              " 'estar',\n",
              " 'estas',\n",
              " 'algunas',\n",
              " 'algo',\n",
              " 'nosotros',\n",
              " 'mi',\n",
              " 'mis',\n",
              " 'tú',\n",
              " 'te',\n",
              " 'ti',\n",
              " 'tu',\n",
              " 'tus',\n",
              " 'ellas',\n",
              " 'nosotras',\n",
              " 'vosotros',\n",
              " 'vosotras',\n",
              " 'os',\n",
              " 'mío',\n",
              " 'mía',\n",
              " 'míos',\n",
              " 'mías',\n",
              " 'tuyo',\n",
              " 'tuya',\n",
              " 'tuyos',\n",
              " 'tuyas',\n",
              " 'suyo',\n",
              " 'suya',\n",
              " 'suyos',\n",
              " 'suyas',\n",
              " 'nuestro',\n",
              " 'nuestra',\n",
              " 'nuestros',\n",
              " 'nuestras',\n",
              " 'vuestro',\n",
              " 'vuestra',\n",
              " 'vuestros',\n",
              " 'vuestras',\n",
              " 'esos',\n",
              " 'esas',\n",
              " 'estoy',\n",
              " 'estás',\n",
              " 'está',\n",
              " 'estamos',\n",
              " 'estáis',\n",
              " 'están',\n",
              " 'esté',\n",
              " 'estés',\n",
              " 'estemos',\n",
              " 'estéis',\n",
              " 'estén',\n",
              " 'estaré',\n",
              " 'estarás',\n",
              " 'estará',\n",
              " 'estaremos',\n",
              " 'estaréis',\n",
              " 'estarán',\n",
              " 'estaría',\n",
              " 'estarías',\n",
              " 'estaríamos',\n",
              " 'estaríais',\n",
              " 'estarían',\n",
              " 'estaba',\n",
              " 'estabas',\n",
              " 'estábamos',\n",
              " 'estabais',\n",
              " 'estaban',\n",
              " 'estuve',\n",
              " 'estuviste',\n",
              " 'estuvo',\n",
              " 'estuvimos',\n",
              " 'estuvisteis',\n",
              " 'estuvieron',\n",
              " 'estuviera',\n",
              " 'estuvieras',\n",
              " 'estuviéramos',\n",
              " 'estuvierais',\n",
              " 'estuvieran',\n",
              " 'estuviese',\n",
              " 'estuvieses',\n",
              " 'estuviésemos',\n",
              " 'estuvieseis',\n",
              " 'estuviesen',\n",
              " 'estando',\n",
              " 'estado',\n",
              " 'estada',\n",
              " 'estados',\n",
              " 'estadas',\n",
              " 'estad',\n",
              " 'he',\n",
              " 'has',\n",
              " 'ha',\n",
              " 'hemos',\n",
              " 'habéis',\n",
              " 'han',\n",
              " 'haya',\n",
              " 'hayas',\n",
              " 'hayamos',\n",
              " 'hayáis',\n",
              " 'hayan',\n",
              " 'habré',\n",
              " 'habrás',\n",
              " 'habrá',\n",
              " 'habremos',\n",
              " 'habréis',\n",
              " 'habrán',\n",
              " 'habría',\n",
              " 'habrías',\n",
              " 'habríamos',\n",
              " 'habríais',\n",
              " 'habrían',\n",
              " 'había',\n",
              " 'habías',\n",
              " 'habíamos',\n",
              " 'habíais',\n",
              " 'habían',\n",
              " 'hube',\n",
              " 'hubiste',\n",
              " 'hubo',\n",
              " 'hubimos',\n",
              " 'hubisteis',\n",
              " 'hubieron',\n",
              " 'hubiera',\n",
              " 'hubieras',\n",
              " 'hubiéramos',\n",
              " 'hubierais',\n",
              " 'hubieran',\n",
              " 'hubiese',\n",
              " 'hubieses',\n",
              " 'hubiésemos',\n",
              " 'hubieseis',\n",
              " 'hubiesen',\n",
              " 'habiendo',\n",
              " 'habido',\n",
              " 'habida',\n",
              " 'habidos',\n",
              " 'habidas',\n",
              " 'soy',\n",
              " 'eres',\n",
              " 'es',\n",
              " 'somos',\n",
              " 'sois',\n",
              " 'son',\n",
              " 'sea',\n",
              " 'seas',\n",
              " 'seamos',\n",
              " 'seáis',\n",
              " 'sean',\n",
              " 'seré',\n",
              " 'serás',\n",
              " 'será',\n",
              " 'seremos',\n",
              " 'seréis',\n",
              " 'serán',\n",
              " 'sería',\n",
              " 'serías',\n",
              " 'seríamos',\n",
              " 'seríais',\n",
              " 'serían',\n",
              " 'era',\n",
              " 'eras',\n",
              " 'éramos',\n",
              " 'erais',\n",
              " 'eran',\n",
              " 'fui',\n",
              " 'fuiste',\n",
              " 'fue',\n",
              " 'fuimos',\n",
              " 'fuisteis',\n",
              " 'fueron',\n",
              " 'fuera',\n",
              " 'fueras',\n",
              " 'fuéramos',\n",
              " 'fuerais',\n",
              " 'fueran',\n",
              " 'fuese',\n",
              " 'fueses',\n",
              " 'fuésemos',\n",
              " 'fueseis',\n",
              " 'fuesen',\n",
              " 'sintiendo',\n",
              " 'sentido',\n",
              " 'sentida',\n",
              " 'sentidos',\n",
              " 'sentidas',\n",
              " 'siente',\n",
              " 'sentid',\n",
              " 'tengo',\n",
              " 'tienes',\n",
              " 'tiene',\n",
              " 'tenemos',\n",
              " 'tenéis',\n",
              " 'tienen',\n",
              " 'tenga',\n",
              " 'tengas',\n",
              " 'tengamos',\n",
              " 'tengáis',\n",
              " 'tengan',\n",
              " 'tendré',\n",
              " 'tendrás',\n",
              " 'tendrá',\n",
              " 'tendremos',\n",
              " 'tendréis',\n",
              " 'tendrán',\n",
              " 'tendría',\n",
              " 'tendrías',\n",
              " 'tendríamos',\n",
              " 'tendríais',\n",
              " 'tendrían',\n",
              " 'tenía',\n",
              " 'tenías',\n",
              " 'teníamos',\n",
              " 'teníais',\n",
              " 'tenían',\n",
              " 'tuve',\n",
              " 'tuviste',\n",
              " 'tuvo',\n",
              " 'tuvimos',\n",
              " 'tuvisteis',\n",
              " 'tuvieron',\n",
              " 'tuviera',\n",
              " 'tuvieras',\n",
              " 'tuviéramos',\n",
              " 'tuvierais',\n",
              " 'tuvieran',\n",
              " 'tuviese',\n",
              " 'tuvieses',\n",
              " 'tuviésemos',\n",
              " 'tuvieseis',\n",
              " 'tuviesen',\n",
              " 'teniendo',\n",
              " 'tenido',\n",
              " 'tenida',\n",
              " 'tenidos',\n",
              " 'tenidas',\n",
              " 'tened']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "texto='Se dice con frecuencia que antes de morir tu vida pasa frente a tus ojos. De hecho esto es cierto. Se le llama vivir.'\n",
        "\n",
        "#convertir la oración a minúsculas\n",
        "texto = texto.lower()\n",
        "\n",
        "#Elimina signos de puntuación y tokeniza\n",
        "palabras = re.findall(r'\\w+', texto,flags = re.UNICODE)\n",
        "\n",
        "#Elimina palabras frecuentes\n",
        "palabras_importantes=[]\n",
        "for palabra in palabras:\n",
        "  if palabra not in stopwords.words('spanish'):\n",
        "    palabras_importantes.append(palabra)\n",
        "\n",
        "print(palabras_importantes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQI7iPhx0NTt",
        "outputId": "c38c88c2-981b-40b0-ad27-095d00f0f683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dice', 'frecuencia', 'morir', 'vida', 'pasa', 'frente', 'ojos', 'hecho', 'cierto', 'llama', 'vivir']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "texto = 'No sabía como se ponía una lavadora hasta que conocí esta y es que es muy sencilla de utilizar! Todo un gustazo cuando estamos aprendiendo'\n",
        "texto_tokenizado = WordPunctTokenizer().tokenize(texto)\n",
        "print(texto_tokenizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33rXfGwM1Irn",
        "outputId": "b3d183f7-74ca-41cf-8fe3-47ef60f3980c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No', 'sabía', 'como', 'se', 'ponía', 'una', 'lavadora', 'hasta', 'que', 'conocí', 'esta', 'y', 'es', 'que', 'es', 'muy', 'sencilla', 'de', 'utilizar', '!', 'Todo', 'un', 'gustazo', 'cuando', 'estamos', 'aprendiendo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "stemmer_text = [stemmer.stem(i) for i in texto_tokenizado]\n",
        "print(stemmer_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFohqVF61m5d",
        "outputId": "11068a9c-5c48-4e7f-aa7e-7059e37f359d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'sab', 'com', 'se', 'pon', 'una', 'lavador', 'hast', 'que', 'conoc', 'esta', 'y', 'es', 'que', 'es', 'muy', 'sencill', 'de', 'utiliz', '!', 'tod', 'un', 'gustaz', 'cuand', 'estam', 'aprend']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = 'When I Grow Up es el cuarto sencillo del disco de Version 2.0, que fue lanzado simultáneamente con el sencillo de The Trick is To Keep Breathing para publicitar la gira europea de la banda.Fue incluida en la banda de sonido de la película Un papá genial protagonizada por Adam Sandler. En Europa fue lanzada en enero de 1999, y se transformó en el quinto Top Ten de la banda en Reino Unido (#9). En Australia fue lanzado en agosto.Los remixes estuvieron a cargo del DJ Danny Tenaglia. Sin embargo, algunas radios estadounidenses hicieron remixes especiales, para lo que cambiaron un poco la letra.'"
      ],
      "metadata": {
        "id": "CgpetEHN1-gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OjiUwbz12k2y",
        "outputId": "81a03a93-25cf-41f6-d44c-c2577c30dbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'When I Grow Up es el cuarto sencillo del disco de Version 2.0, que fue lanzado simultáneamente con el sencillo de The Trick is To Keep Breathing para publicitar la gira europea de la banda.Fue incluida en la banda de sonido de la película Un papá genial protagonizada por Adam Sandler. En Europa fue lanzada en enero de 1999, y se transformó en el quinto Top Ten de la banda en Reino Unido (#9). En Australia fue lanzado en agosto.Los remixes estuvieron a cargo del DJ Danny Tenaglia. Sin embargo, algunas radios estadounidenses hicieron remixes especiales, para lo que cambiaron un poco la letra.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "texto_tokenizado = WordPunctTokenizer().tokenize(texto)\n",
        "print(texto_tokenizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxLYL1hK2rhQ",
        "outputId": "365cf6cf-97fd-4341-d65e-c6771632523b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'I', 'Grow', 'Up', 'es', 'el', 'cuarto', 'sencillo', 'del', 'disco', 'de', 'Version', '2', '.', '0', ',', 'que', 'fue', 'lanzado', 'simultáneamente', 'con', 'el', 'sencillo', 'de', 'The', 'Trick', 'is', 'To', 'Keep', 'Breathing', 'para', 'publicitar', 'la', 'gira', 'europea', 'de', 'la', 'banda', '.', 'Fue', 'incluida', 'en', 'la', 'banda', 'de', 'sonido', 'de', 'la', 'película', 'Un', 'papá', 'genial', 'protagonizada', 'por', 'Adam', 'Sandler', '.', 'En', 'Europa', 'fue', 'lanzada', 'en', 'enero', 'de', '1999', ',', 'y', 'se', 'transformó', 'en', 'el', 'quinto', 'Top', 'Ten', 'de', 'la', 'banda', 'en', 'Reino', 'Unido', '(#', '9', ').', 'En', 'Australia', 'fue', 'lanzado', 'en', 'agosto', '.', 'Los', 'remixes', 'estuvieron', 'a', 'cargo', 'del', 'DJ', 'Danny', 'Tenaglia', '.', 'Sin', 'embargo', ',', 'algunas', 'radios', 'estadounidenses', 'hicieron', 'remixes', 'especiales', ',', 'para', 'lo', 'que', 'cambiaron', 'un', 'poco', 'la', 'letra', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lista de todos los bigramas\n",
        "esBigramas= nltk.ngrams(texto_tokenizado,2)\n",
        "list(esBigramas)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL8BBBjw29QX",
        "outputId": "0428c340-3179-4740-d893-3e8a6b85cbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('When', 'I'),\n",
              " ('I', 'Grow'),\n",
              " ('Grow', 'Up'),\n",
              " ('Up', 'es'),\n",
              " ('es', 'el'),\n",
              " ('el', 'cuarto'),\n",
              " ('cuarto', 'sencillo'),\n",
              " ('sencillo', 'del'),\n",
              " ('del', 'disco'),\n",
              " ('disco', 'de')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lista de todos los bigramas\n",
        "esTrigramas = nltk.ngrams(texto_tokenizado,3)\n",
        "list(esTrigramas)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TekDgW4M3qE6",
        "outputId": "6dcc9294-e6ec-4118-b7e0-b927d303722f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('When', 'I', 'Grow'),\n",
              " ('I', 'Grow', 'Up'),\n",
              " ('Grow', 'Up', 'es'),\n",
              " ('Up', 'es', 'el'),\n",
              " ('es', 'el', 'cuarto'),\n",
              " ('el', 'cuarto', 'sencillo'),\n",
              " ('cuarto', 'sencillo', 'del'),\n",
              " ('sencillo', 'del', 'disco'),\n",
              " ('del', 'disco', 'de'),\n",
              " ('disco', 'de', 'Version')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHW7AsxG4vnf",
        "outputId": "72043ffa-6444-475d-ba31-17577ca724c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams,trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n"
      ],
      "metadata": {
        "id": "Hpaf8SlJ31b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables para el modelo\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "#Conteo de frecuencia de co-ocurrencias\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        " \n",
        "# Convertir el conteo en probabilidades\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "JpRWzhkI4DuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "sorted(dict(model['today','the']).items(), key=operator.itemgetter(1),reverse=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrMZPbwq6KJg",
        "outputId": "fc4e40c8-ba52-4d69-e10f-e4c0efbf808c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('company', 0.16666666666666666),\n",
              " ('price', 0.1111111111111111),\n",
              " ('public', 0.05555555555555555),\n",
              " ('European', 0.05555555555555555),\n",
              " ('Bank', 0.05555555555555555),\n",
              " ('emirate', 0.05555555555555555),\n",
              " ('overseas', 0.05555555555555555),\n",
              " ('newspaper', 0.05555555555555555),\n",
              " ('Turkish', 0.05555555555555555),\n",
              " ('increase', 0.05555555555555555),\n",
              " ('options', 0.05555555555555555),\n",
              " ('Higher', 0.05555555555555555),\n",
              " ('pound', 0.05555555555555555),\n",
              " ('Italian', 0.05555555555555555),\n",
              " ('time', 0.05555555555555555)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE"
      ],
      "metadata": {
        "id": "x8Lzqd8n6fzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = ['una manzana','una pera']\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent)))\n",
        "                   for sent in train_sentences]\n",
        "\n",
        "n = 1\n",
        "train_data, padded_vocab = padded_everygram_pipeline(n, tokenized_text)\n",
        "model = MLE(n)\n",
        "model.fit(train_data, padded_vocab)\n",
        "\n",
        "test_sentences = ['una manzana', 'un gusano','manzana','La manzana roja es de mi casa junto con la pera','apple']\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent)))\n",
        "                  for sent in test_sentences]\n",
        "\n",
        "test_Data,_= padded_everygram_pipeline(n,tokenized_text)\n",
        "for test in test_Data:\n",
        "  print(\"MLE Estima\",[((ngram[-1],ngram[:-1]), model.score(ngram[-1],ngram[:-1])) for ngram in test])\n",
        "\n",
        "test_data,_ = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "for i, test in enumerate(test_data):\n",
        "  print(\"PP({0}):{1}\".format(test_sentences[i],model.perplexity(test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH-giEIr6r8l",
        "outputId": "5c0edc31-f5ce-4b5b-9000-2ed2c251f4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE Estima [(('una', ()), 0.5), (('manzana', ()), 0.25)]\n",
            "MLE Estima [(('un', ()), 0.0), (('gusano', ()), 0.0)]\n",
            "MLE Estima [(('manzana', ()), 0.25)]\n",
            "MLE Estima [(('la', ()), 0.0), (('manzana', ()), 0.25), (('roja', ()), 0.0), (('es', ()), 0.0), (('de', ()), 0.0), (('mi', ()), 0.0), (('casa', ()), 0.0), (('junto', ()), 0.0), (('con', ()), 0.0), (('la', ()), 0.0), (('pera', ()), 0.25)]\n",
            "MLE Estima [(('apple', ()), 0.0)]\n",
            "PP(una manzana):2.8284271247461903\n",
            "PP(un gusano):inf\n",
            "PP(manzana):4.0\n",
            "PP(La manzana roja es de mi casa junto con la pera):inf\n",
            "PP(apple):inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asegúrate de:\n",
        "\n",
        " \n",
        "\n",
        "*   Realizar ejercicios prácticos que incluyan el proceso de clasificación de textos.\n",
        "*   Comprender la diferencia entre conjunto de entrenamiento y de pruebas.\n",
        "\n",
        "*   Diferenciar los pasos del proceso de clasificación. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aVy7yt48FYLI"
      }
    }
  ]
}